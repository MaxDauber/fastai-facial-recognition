{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import dlib\n",
    "import subprocess\n",
    "import random\n",
    "import os\n",
    "bs=8\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring path of dataset\n",
    "path_img = Path('/root/model_training/intern_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create list of cv2-loaded images in a given directory\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename),1)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "#function to convert cv2-loaded image into RGB\n",
    "def convertToRGB(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def scale_image(image):\n",
    "    scale_percent = 25 # percent of original size\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    # resize image\n",
    "    return cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def detectFaceOpenCVDnn(net, image):\n",
    "    frameHeight = image.shape[0]\n",
    "    frameWidth = image.shape[1]\n",
    "    data = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    \n",
    "    net.setInput(data)\n",
    "    detections = net.forward()\n",
    "    bounding_boxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bounding_boxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return image, bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded images for sam\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "loaded images for bilal\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n",
      "Faces found:  1\n"
     ]
    }
   ],
   "source": [
    "# model to use to locate faces\n",
    "# haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "conf_threshold = 0.95\n",
    "DNN = \"CAFFE\"\n",
    "if DNN == \"CAFFE\":\n",
    "    modelFile = \"models/weights.caffemodel\"\n",
    "    configFile = \"models/deploy.prototxt\"\n",
    "    net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "else:\n",
    "    modelFile = \"opencv_face_detector_uint8.pb\"\n",
    "    configFile = \"opencv_face_detector.pbtxt\"\n",
    "    net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n",
    "\n",
    "    \n",
    "for dirname, dirnames, filenames in os.walk('./intern_images'):\n",
    "    for subdirname in dirnames:\n",
    "        #print(os.path.join(dirname, subdirname))\n",
    "        imgs = load_images_from_folder(os.path.join(dirname, subdirname))\n",
    "        print('loaded images for '+ subdirname)\n",
    "        for img in imgs:\n",
    "            # faces = haar_cascade.detectMultiScale(img, scaleFactor = 1.2, minNeighbors = 4, minSize=(500, 500))\n",
    "            overlay_image, faces = detectFaceOpenCVDnn(net,img)\n",
    "            print('Faces found: ', len(faces))\n",
    "            #for (x,y,w,h) in faces: cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            plt.figure()\n",
    "            plt.imshow(convertToRGB(overlay_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_raw = cv2.imread(path_img+'max/max_001.JPG',1)\n",
    "type(img_raw)\n",
    "img_raw.shape\n",
    "plt.imshow(img_raw)\n",
    "cv2.imwrite(os.path.join(path , 'waka.jpg'), img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## applying transforms\n",
    "tfms_list = [rotate(degrees=270, p=1),jitter(magnitude=(random.randrange(-3,3)/100), p=0.25), \n",
    "            contrast(scale=(0.5, 2.), p=0.5), brightness(change=(0.1, 0.9), p=0.5)]\n",
    "tfms_list = [rotate(degrees=270, p=1)]\n",
    "tfms = [tfms_list, tfms_list]\n",
    "\n",
    "## Loading data \n",
    "data = ImageDataBunch.from_folder(path=path_img, train='/', valid_pct=0.07, ds_tfms=tfms, bs=bs, size=(540,720))\n",
    "\n",
    "## Normalizing data based on ImageNet parameters\n",
    "data.normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.classes)\n",
    "len(data.classes),data.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "face_detector = dlib.get_frontal_face_detector()\n",
    "for file_name in data.items:\n",
    "    img = dlib.load_rgb_image(str(file_name))\n",
    "    detected_faces = face_detector(img)\n",
    "    print(\"Found {} faces in the image file {}\".format(len(detected_faces), file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex(): return open_image(path_img/'max/max_001.JPG')\n",
    "\n",
    "def plots_f(rows, cols, width, height, **kwargs):\n",
    "    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n",
    "        rows,cols,figsize=(width,height))[1].flatten())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## To create a ResNET 50 with pretrained weights\n",
    "learn = cnn_learner(data, models.resnet50, metrics=accuracy)\n",
    "print(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training results:  ' + str(learn.validate(learn.data.train_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation results: ' + str(learn.validate(learn.data.valid_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y,losses = learn.get_preds(with_loss=True)\n",
    "interp = ClassificationInterpretation(learn, preds, y, losses)\n",
    "\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_top_losses(4, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(5,5), dpi=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
